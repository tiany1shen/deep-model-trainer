experiment_name: MLP-ExampleDataset
trial_index: 1
use_cpu: false
mixed_precision: null
gradient_accumulation_steps: 1
split_batches: true
log_metrics: true

model:
  name: MLP
  params:
    input_dim: 2
    hidden_dims: 
      - 16
      - 32
    output_dim: 3
  checkpoint_path: experiments/MLP-ExampleDataset/trial-1/checkpoints/epoch-20.pth


dataset:
  train:
    name: ExampleDataset
    params:
      # size: 10000
      data_path: data/example-10k.json
    
  eval:
    name: ExampleDataset
    params:
      # size: 100
      data_path: data/example-100.json
    

train:
  num_epochs: 20
  batch_size: 400
  optimizer: Adam
  learning_rate: 0.01
  use_ema: false
  need_other_modes:
    - eval
  need_datasets:
    - train
    - eval
  
  log_interval: 1 # step
  eval_interval: 1 # epoch
  save_interval: 1 # epoch

eval:
  batch_size: 100
  # need_other_modes:
  #   - null
  need_datasets:
    - eval

sample:
  size: 64


ema:
  decay: 0.9999
  checkpoint_path: null