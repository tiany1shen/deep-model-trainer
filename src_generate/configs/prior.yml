#* fill experiment info
experiment_name: MNIST-Generate-Prior
trial_index: 1
trainer_name: GptTrainer

use_cpu: false
gradient_accumulation_steps: 2
split_batches: true
log_metrics: true

#* model config
model:
  name: GPT
  params:
    num_class: 10
    num_vocab: 128
    sen_len: 49
    num_layer: 6
    num_head: 4
    dim: 256

  checkpoint_path: " "

encoder:
  name: VQVAE
  params:
    encoder:
      input_dim: 1
      output_dim: 1
      hidden_dims:
        - 16
        - 32
    decoder:
      input_dim: 1
      output_dim: 1
      hidden_dims:
        - 32
        - 16
    quantizer:
      codebook_dim: 1
      codebook_num: 128
      decay: 0.99
      reset_threshold: 0.01
      epsilon: 1.0e-5

  checkpoint_path: experiments/MNIST-Generate-VQVAE/train-1/checkpoints/epoch-100.pth

#* dataset config
dataset:
  train:
    name: MNIST
    params:
      root: "data"
      train: true
    
  eval:
    name: MNIST
    params:
      root: "data"
      train: false

#* training config
train:
  num_epochs: 200
  batch_size: 1024
  optimizer: Adam
  learning_rate: 0.001
  use_ema: false
  need_other_modes:
    - eval
  need_datasets:
    - train
  
  loss:
    names: 
      - Class
      - Predict
    weights: 
      - 1.0
      - 1.0
  metric_names:

  log_interval: 1 # step
  eval_interval: 10 # epoch
  save_interval: 10 # epoch

#* evaluating config
eval:
  batch_size: 2
  # need_other_modes:
  #   - null
  need_datasets:
    - eval
  metric_names: 


#* sampling config
sample:
  size: 10

#* other flexible config
# ema:
#   decay: 0.9999
#   checkpoint_path: null